{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IAM Dataset Analysis\n",
    "\n",
    "Set up the environment and import all the necessary libraries and modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from iam import bezier_curves\n",
    "\n",
    "LINE_STROKES_DATA_DIR = Path(\"../../datasets/IAM/lineStrokes\")\n",
    "LINE_LABELS_DATA_DIR = Path(\"../../datasets/IAM/ascii\")\n",
    "LINE_IMAGES_DATA_DIR = Path(\"../../datasets/IAM/lineImages\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the functions used to extract the data from the IAM dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_line_from_labels_file(\n",
    "    labels_file: Path, line_idx: int\n",
    ") -> Optional[str]:\n",
    "    \"\"\"Gets the line label from the labels file.\n",
    "\n",
    "    Args:\n",
    "        labels_file (Path): The labels file path.\n",
    "        line_idx (int): The line index.\n",
    "\n",
    "    Returns:\n",
    "        The line label. None if the line label is not found.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the labels file is invalid.\n",
    "        FileNotFoundError: If the labels file is not found.\n",
    "        ValueError: If the labels file is not a file.\n",
    "        ValueError: If the line label index is not a number.\n",
    "    \"\"\"\n",
    "    # Check if the labels file is valid.\n",
    "    if not isinstance(labels_file, Path):\n",
    "        raise ValueError(\n",
    "            f\"Invalid labels file: {labels_file}. Must be an instance of {Path}.\"\n",
    "        )\n",
    "\n",
    "    # Check if the labels file exists.\n",
    "    if not labels_file.exists():\n",
    "        raise FileNotFoundError(f\"Labels file not found: {labels_file}.\")\n",
    "\n",
    "    # Check if the labels file is a file.\n",
    "    if not labels_file.is_file():\n",
    "        raise ValueError(f\"Labels file is not a file: {labels_file}.\")\n",
    "\n",
    "    # Check if the line label index is valid.\n",
    "    if not isinstance(line_idx, int):\n",
    "        raise ValueError(\n",
    "            f\"Invalid line label index: {line_idx}. Must be an integer.\"\n",
    "        )\n",
    "\n",
    "    # Get the line label from the labels file.\n",
    "    with open(labels_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        # The index of the line where the line labels start (CSR line).\n",
    "        labels_start_idx = 0\n",
    "\n",
    "        for i, line in enumerate(file):\n",
    "            # Check if we have found CSR line and save the index.\n",
    "            if line.startswith(\"CSR:\"):\n",
    "                labels_start_idx = i\n",
    "\n",
    "            if (\n",
    "                labels_start_idx > 0  # Ensure we have found the CSR line.\n",
    "                and i\n",
    "                == labels_start_idx\n",
    "                + line_idx\n",
    "                + 1  # Add 1 to skip the blank line after the CSR line.\n",
    "            ):\n",
    "                # Only keep the characters that are in the IAM dataset.\n",
    "                return \"\".join(\n",
    "                    [char for char in line if char.isalnum() or char == \" \"]\n",
    "                )\n",
    "\n",
    "    # If we have not found the line label, return None.\n",
    "    return None\n",
    "\n",
    "\n",
    "def filter_and_get_stroke_file_paths(\n",
    "    *, root: str, stroke_files: list[str]\n",
    ") -> list[Path]:\n",
    "    \"\"\"Filter the stroke files and return a list of pathlib.Path objects representing the files.\n",
    "\n",
    "    Args:\n",
    "        root (str): The root directory of the stroke files.\n",
    "        stroke_files (list[str]): A list of file names of the stroke files.\n",
    "\n",
    "    Returns:\n",
    "        list[Path]: A list of pathlib.Path objects representing the stroke files.\n",
    "    \"\"\"\n",
    "    stroke_file_paths = [\n",
    "        # Get the full path to the stroke file.\n",
    "        Path(root) / Path(file)\n",
    "        for file in stroke_files\n",
    "        if file.endswith(\".xml\")\n",
    "        # Don't include these files. They are invalid or missing data.\n",
    "        and not file.startswith(\n",
    "            \"z01-000z\"\n",
    "        )  # Exclude files starting with \"z01-000z\".\n",
    "        and not file.startswith(\n",
    "            \"a08-551z-08\"\n",
    "        )  # Exclude files starting with \"a08-551z-08\".\n",
    "        and not file.startswith(\n",
    "            \"a08-551z-09\"\n",
    "        )  # Exclude files starting with \"a08-551z-09\".\n",
    "    ]\n",
    "\n",
    "    assert all(\n",
    "        not (\n",
    "            file.name.startswith(\"z01-000z\")\n",
    "            and file.name.startswith(\"a08-551z-08\")\n",
    "            and file.name.startswith(\"a08-551z-09\")\n",
    "        )\n",
    "        for file in stroke_file_paths\n",
    "    ), (\n",
    "        \"Invalid stroke files. \"\n",
    "        'The stroke files must not start with \"z01-000z\", \"a08-551z-08\", or \"a08-551z-09\".'\n",
    "    )\n",
    "\n",
    "    return stroke_file_paths\n",
    "\n",
    "\n",
    "def get_label_from_stroke_file(\n",
    "    stroke_file: Path,\n",
    ") -> tuple[Optional[str], str, Path]:\n",
    "    \"\"\"Gets the line label and the labels file name that the label belongs to.\n",
    "\n",
    "    Args:\n",
    "        stroke_file (Path): The stroke file path.\n",
    "\n",
    "    Returns:\n",
    "        The line label, the label file name, and the image file path.\n",
    "        None if the line label is not found.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the stroke file is invalid.\n",
    "        FileNotFoundError: If the stroke file is not found.\n",
    "        ValueError: If the stroke file is not an XML file.\n",
    "        ValueError: If the line label index is not a number.\n",
    "    \"\"\"\n",
    "    # Check if the stroke file is valid.\n",
    "    if not isinstance(stroke_file, Path):\n",
    "        raise ValueError(\n",
    "            f\"Invalid stroke file: {stroke_file}. Must be an instance of {Path}.\"\n",
    "        )\n",
    "\n",
    "    if not stroke_file.is_file():\n",
    "        raise FileNotFoundError(f\"Stroke file not found: {stroke_file}.\")\n",
    "\n",
    "    if stroke_file.suffix != \".xml\":\n",
    "        raise ValueError(\n",
    "            f\"Invalid stroke file: {stroke_file}. Must be an XML file.\"\n",
    "        )\n",
    "\n",
    "    # Get the stroke file name without the extension.\n",
    "    stroke_file_name = stroke_file.stem  # e.g. a01-000u-01\n",
    "\n",
    "    # Get the line label index.\n",
    "    line_label_idx_chars = stroke_file_name[-2:]  # e.g. 01\n",
    "    if not line_label_idx_chars.isdigit():\n",
    "        raise ValueError(\n",
    "            f\"Invalid stroke file: {stroke_file}. \"\n",
    "            f\"Label line index must be a number.\"\n",
    "        )\n",
    "    line_label_idx = int(line_label_idx_chars)\n",
    "\n",
    "    # Get the labels file directory and name.\n",
    "    root_labels_dir = stroke_file_name[:3]  # e.g. a01\n",
    "    sub_labels_dir = stroke_file_name[:7]  # e.g. a01-000\n",
    "    labels_file_name = stroke_file_name[:-3]  # e.g. a01-000u\n",
    "\n",
    "    # Get the labels file path. e.g. ../datasets/IAM/ascii/a01/a01-000/a01-000u.txt\n",
    "    labels_file = (\n",
    "        LINE_LABELS_DATA_DIR\n",
    "        / root_labels_dir\n",
    "        / sub_labels_dir\n",
    "        / f\"{labels_file_name}.txt\"\n",
    "    )\n",
    "\n",
    "    image_file = (\n",
    "        LINE_IMAGES_DATA_DIR\n",
    "        / root_labels_dir\n",
    "        / sub_labels_dir\n",
    "        / f\"{labels_file_name}-{line_label_idx_chars}.tif\"\n",
    "    )\n",
    "    assert image_file.exists(), f\"Image file not found: {image_file}.\"\n",
    "\n",
    "    # Get the line label from the labels file.\n",
    "    return (\n",
    "        get_line_from_labels_file(labels_file, line_label_idx),\n",
    "        labels_file_name,\n",
    "        image_file,\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_stroke_element(stroke_elem: ET.Element) -> bezier_curves.StrokeData:\n",
    "    \"\"\"Parses the stroke element.\n",
    "\n",
    "    Args:\n",
    "        stroke (ET.Element): The stroke element.\n",
    "\n",
    "    Returns:\n",
    "        The stroke data, which contains the x points, y points, time stamps,\n",
    "        and pen_ups information.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the stroke element is invalid.\n",
    "    \"\"\"\n",
    "    # Check if the stroke element is valid.\n",
    "    if not isinstance(stroke_elem, ET.Element):\n",
    "        raise ValueError(\n",
    "            f\"Invalid stroke ET element: {stroke_elem}. Must be an instance of {ET.Element}.\"\n",
    "        )\n",
    "\n",
    "    x_points = []\n",
    "    y_points = []\n",
    "    time_stamps = []\n",
    "    pen_ups = []\n",
    "    fst_timestamp = 0\n",
    "\n",
    "    # Go through each point in the stroke.\n",
    "    for i, point in enumerate(stroke_elem.findall(\"Point\")):\n",
    "        x_points.append(float(point.attrib[\"x\"]))\n",
    "        y_points.append(float(point.attrib[\"y\"]))\n",
    "\n",
    "        if not time_stamps:\n",
    "            # Ensure first time stamp is 0, not the actual time stamp.\n",
    "            fst_timestamp = float(point.attrib[\"time\"])\n",
    "            time_stamps.append(0)\n",
    "        else:\n",
    "            time_stamps.append(float(point.attrib[\"time\"]) - fst_timestamp)\n",
    "\n",
    "        if i == len(stroke_elem.findall(\"Point\")) - 1:\n",
    "            # If this is the last point in the stroke, the pen is up.\n",
    "            pen_ups.append(1)\n",
    "        else:\n",
    "            # If this is not the last point in the stroke, the pen is down.\n",
    "            pen_ups.append(0)\n",
    "\n",
    "    assert (\n",
    "        len(x_points) == len(y_points) == len(time_stamps) == len(pen_ups)\n",
    "    ), (\n",
    "        f\"Invalid stroke element: {stroke_elem}. \"\n",
    "        \"The number of x points, y points, time stamps, and pen ups must be equal.\"\n",
    "    )\n",
    "\n",
    "    return bezier_curves.StrokeData(x_points, y_points, time_stamps, pen_ups)\n",
    "\n",
    "\n",
    "def get_strokes_from_stroke_file(\n",
    "    stroke_file: Path,\n",
    ") -> list[bezier_curves.StrokeData]:\n",
    "    \"\"\"Gets the list of stroke data from the stroke file.\n",
    "\n",
    "    Args:\n",
    "        stroke_file (Path): The stroke file path.\n",
    "\n",
    "    Returns:\n",
    "        The list of stroke data.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the stroke file is invalid.\n",
    "        FileNotFoundError: If the stroke file is not found.\n",
    "        ValueError: If the stroke file is not an XML file.\n",
    "    \"\"\"\n",
    "    # Check if the stroke file is valid.\n",
    "    if not isinstance(stroke_file, Path):\n",
    "        raise ValueError(\n",
    "            f\"Invalid stroke file: {stroke_file}. Must be an instance of {Path}.\"\n",
    "        )\n",
    "\n",
    "    if not stroke_file.is_file():\n",
    "        raise FileNotFoundError(f\"Stroke file not found: {stroke_file}.\")\n",
    "\n",
    "    if stroke_file.suffix != \".xml\":\n",
    "        raise ValueError(\n",
    "            f\"Invalid stroke file: {stroke_file}. Must be an XML file.\"\n",
    "        )\n",
    "\n",
    "    tree = ET.parse(stroke_file)\n",
    "    root = tree.getroot()\n",
    "    stroke_set = root.find(\"StrokeSet\")\n",
    "\n",
    "    strokes = [\n",
    "        parse_stroke_element(stroke) for stroke in stroke_set.findall(\"Stroke\")\n",
    "    ]\n",
    "\n",
    "    # Normalize the strokes.\n",
    "    max_y = float(\n",
    "        root.find(\"WhiteboardDescription/DiagonallyOppositeCoords\").attrib[\"y\"]\n",
    "    )\n",
    "\n",
    "    for stroke in strokes:\n",
    "        min_x = min(stroke.x_points)\n",
    "        min_y = min(stroke.y_points)\n",
    "\n",
    "        # Shift x and y points to start at 0.\n",
    "        stroke.x_points = [x - min_x for x in stroke.x_points]\n",
    "        stroke.y_points = [y - min_y for y in stroke.y_points]\n",
    "\n",
    "        # Scale points so that the y_points are between 0 and 1. The x_points will be scaled\n",
    "        # by the same amount, to preserve the aspect ratio.\n",
    "        scale = (\n",
    "            1 if max_y - min_y == 0 else 1 / (max_y - min_y)\n",
    "        )  # Avoid division by 0.\n",
    "\n",
    "        stroke.x_points = [x * scale for x in stroke.x_points]\n",
    "        stroke.y_points = [y * scale for y in stroke.y_points]\n",
    "\n",
    "        assert all(0 <= y <= 1 for y in stroke.y_points), (\n",
    "            f\"Invalid stroke file: {stroke_file}. \"\n",
    "            f\"All y-points must be between 0 and 1.\"\n",
    "        )\n",
    "\n",
    "    assert all(\n",
    "        len(stroke.x_points)\n",
    "        == len(stroke.y_points)\n",
    "        == len(stroke.time_stamps)\n",
    "        == len(stroke.pen_ups)\n",
    "        for stroke in strokes\n",
    "    ), (\n",
    "        f\"Invalid stroke file: {stroke_file}. \"\n",
    "        \"The number of x points, y points, time stamps, and pen ups must be equal.\"\n",
    "    )\n",
    "\n",
    "    return strokes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all data from the IAM dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "\n",
    "for root, _, stroke_files in os.walk(LINE_STROKES_DATA_DIR):\n",
    "    # Get all stroke files in the directory.\n",
    "    stroke_files = filter_and_get_stroke_file_paths(\n",
    "        root=root, stroke_files=stroke_files\n",
    "    )\n",
    "\n",
    "    # Go through each stroke file in the directory.\n",
    "    for stroke_file in stroke_files:\n",
    "        # Get the label for the stroke file.\n",
    "        (\n",
    "            line_label,\n",
    "            labels_file_name,\n",
    "            label_image_file,\n",
    "        ) = get_label_from_stroke_file(stroke_file)\n",
    "\n",
    "        if line_label is None:\n",
    "            break\n",
    "\n",
    "        # Get the strokes from the stroke file.\n",
    "        strokes = get_strokes_from_stroke_file(stroke_file)\n",
    "        files.append(\n",
    "            (\n",
    "                line_label,\n",
    "                labels_file_name,\n",
    "                label_image_file,\n",
    "                stroke_file,\n",
    "                len(strokes),\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the dataset types and a function to set up the data stores for each dataset type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetType(Enum):\n",
    "    \"\"\"An enum for the dataset types.\"\"\"\n",
    "\n",
    "    TRAIN_CROSS_VAL = Path(\n",
    "        \"../../datasets/IAM/trainset.txt\"\n",
    "    )  # Used during cross validation.\n",
    "    VAL_1 = Path(\"../../datasets/IAM/valset1.txt\")\n",
    "    VAL_2 = Path(\"../../datasets/IAM/valset2.txt\")\n",
    "    TEST = Path(\"../../datasets/IAM/testset.txt\")\n",
    "    TRAIN_SINGLE_VAL = None  # Train and val_1 combined. Used during validation on single dataset.\n",
    "\n",
    "\n",
    "def set_up_train_val_test_data_stores() -> (\n",
    "    tuple[\n",
    "        bezier_curves.BezierData,\n",
    "        bezier_curves.BezierData,\n",
    "        bezier_curves.BezierData,\n",
    "        bezier_curves.BezierData,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Set up the data stores for the train, first validation, second validation, and test sets.\n",
    "\n",
    "    Returns:\n",
    "        tuple[bezier_curves.BezierData, bezier_curves.BezierData, bezier_curves.BezierData, bezier_curves.BezierData]: A tuple of bezier_curves.BezierData objects for\n",
    "            the train, first validation, second validation, and test sets.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the train, first validation, second validation, and test sets are not\n",
    "            disjoint.\n",
    "    \"\"\"\n",
    "\n",
    "    def get_train_val_test_label_file_names() -> (\n",
    "        tuple[set[str], set[str], set[str], set[str]]\n",
    "    ):\n",
    "        \"\"\"Get the label file names for train, first validation, second validation, and test sets.\n",
    "\n",
    "        Returns:\n",
    "            tuple[set[str], set[str], set[str], set[str]]: A tuple of sets of label file names for\n",
    "                train, first validation, second validation, and test sets.\n",
    "        \"\"\"\n",
    "\n",
    "        def _get_dataset_label_file_names(\n",
    "            dataset_type: DatasetType,\n",
    "        ) -> set[str]:\n",
    "            \"\"\"Get the label file names for the given dataset.\n",
    "\n",
    "            Args:\n",
    "                dataset_type (DatasetType): The dataset type.\n",
    "\n",
    "            Returns:\n",
    "                set[str]: A set of label file names for the given dataset.\n",
    "            \"\"\"\n",
    "            label_file_names = set()\n",
    "            with open(\n",
    "                dataset_type.value, \"r\", encoding=\"utf-8\"\n",
    "            ) as label_files:\n",
    "                for label_file_name in label_files:\n",
    "                    label_file_names.add(label_file_name.strip())\n",
    "            return label_file_names\n",
    "\n",
    "        train_cross_val_data_file_names = _get_dataset_label_file_names(\n",
    "            DatasetType.TRAIN_CROSS_VAL\n",
    "        )\n",
    "        val_1_data_file_names = _get_dataset_label_file_names(\n",
    "            DatasetType.VAL_1\n",
    "        )\n",
    "        val_2_data_file_names = _get_dataset_label_file_names(\n",
    "            DatasetType.VAL_2\n",
    "        )\n",
    "        test_data_file_names = _get_dataset_label_file_names(DatasetType.TEST)\n",
    "\n",
    "        # Check that the train, first validation, second validation, and test sets are disjoint.\n",
    "        if not train_cross_val_data_file_names.isdisjoint(\n",
    "            val_1_data_file_names\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"The train and first validation sets are not disjoint.\"\n",
    "            )\n",
    "        if not train_cross_val_data_file_names.isdisjoint(\n",
    "            val_2_data_file_names\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"The train and second validation sets are not disjoint.\"\n",
    "            )\n",
    "        if not train_cross_val_data_file_names.isdisjoint(\n",
    "            test_data_file_names\n",
    "        ):\n",
    "            raise ValueError(\"The train and test sets are not disjoint.\")\n",
    "        if not val_1_data_file_names.isdisjoint(val_2_data_file_names):\n",
    "            raise ValueError(\n",
    "                \"The first and second validation sets are not disjoint.\"\n",
    "            )\n",
    "        if not val_1_data_file_names.isdisjoint(test_data_file_names):\n",
    "            raise ValueError(\n",
    "                \"The first validation and test sets are not disjoint.\"\n",
    "            )\n",
    "        if not val_2_data_file_names.isdisjoint(test_data_file_names):\n",
    "            raise ValueError(\n",
    "                \"The second validation and test sets are not disjoint.\"\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            train_cross_val_data_file_names,\n",
    "            val_1_data_file_names,\n",
    "            val_2_data_file_names,\n",
    "            test_data_file_names,\n",
    "        )\n",
    "\n",
    "    (\n",
    "        train_cross_val_label_file_names,\n",
    "        val_1_label_file_names,\n",
    "        val_2_label_file_names,\n",
    "        test_label_file_names,\n",
    "    ) = get_train_val_test_label_file_names()\n",
    "\n",
    "    train_cross_val_data = bezier_curves.BezierData(\n",
    "        label_file_names=train_cross_val_label_file_names\n",
    "    )  # The train dataset used for cross validation.\n",
    "\n",
    "    val_1_data = bezier_curves.BezierData(\n",
    "        label_file_names=val_1_label_file_names\n",
    "    )  # The first validation dataset.\n",
    "\n",
    "    val_2_data = bezier_curves.BezierData(\n",
    "        label_file_names=val_2_label_file_names\n",
    "    )  # The second validation dataset.\n",
    "\n",
    "    test_data = bezier_curves.BezierData(\n",
    "        label_file_names=test_label_file_names\n",
    "    )  # The test dataset.\n",
    "\n",
    "    return train_cross_val_data, val_1_data, val_2_data, test_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data files for the train, first validation, second validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_cross_val_data,\n",
    "    val_1_data,\n",
    "    val_2_data,\n",
    "    test_data,\n",
    ") = set_up_train_val_test_data_stores()\n",
    "\n",
    "train_cross_val_data_files = []\n",
    "val_1_data_files = []\n",
    "val_2_data_files = []\n",
    "test_data_files = []\n",
    "\n",
    "for i, (_, labels_file_name, _, _, _) in enumerate(files):\n",
    "    if labels_file_name in train_cross_val_data.label_file_names:\n",
    "        train_cross_val_data_files.append(files[i])\n",
    "    elif labels_file_name in val_1_data.label_file_names:\n",
    "        val_1_data_files.append(files[i])\n",
    "    elif labels_file_name in val_2_data.label_file_names:\n",
    "        val_2_data_files.append(files[i])\n",
    "    elif labels_file_name in test_data.label_file_names:\n",
    "        test_data_files.append(files[i])\n",
    "\n",
    "print(\n",
    "    f\"Total files: {len(files)}\\n\"\n",
    "    f\"Train files: {len(train_cross_val_data_files)}\\n\"\n",
    "    f\"Val_1 files: {len(val_1_data_files)}\\n\"\n",
    "    f\"Val_2 files: {len(val_2_data_files)}\\n\"\n",
    "    f\"Test files: {len(test_data_files)}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to display samples from the given files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples_from_files(\n",
    "    num_samples: int, files: list[tuple], title: str\n",
    "):\n",
    "    \"\"\"Display samples from the given files.\n",
    "\n",
    "    Args:\n",
    "        num_samples (int): The number of samples to display.\n",
    "        files (list[tuple]): The list of files to display samples from.\n",
    "        title (str): The title of the plot.\n",
    "    \"\"\"\n",
    "    # Get a random sample of files.\n",
    "    random_indices = np.random.choice(\n",
    "        len(files), size=num_samples, replace=False\n",
    "    )\n",
    "\n",
    "    # Calculate the number of rows and columns for the subplots.\n",
    "    num_rows = math.ceil(num_samples / 2)\n",
    "    num_cols = 2 if num_samples > 1 else 1\n",
    "\n",
    "    # Create a grid of subplots.\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 2))\n",
    "\n",
    "    # If there's only one sample, axs is not a list. Make it a list for consistency.\n",
    "    if num_samples == 1:\n",
    "        axs = [axs]\n",
    "\n",
    "    # Set the title of the plot to be at the top.\n",
    "    fig.subplots_adjust(top=0.97)\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "\n",
    "    # Display the samples.\n",
    "    for i, idx in enumerate(random_indices):\n",
    "        (\n",
    "            line_label,\n",
    "            _,\n",
    "            label_image_file,\n",
    "            _,\n",
    "            num_strokes,\n",
    "        ) = files[idx]\n",
    "\n",
    "        # Display the stroke image.\n",
    "        axs[i // 2, i % 2].imshow(plt.imread(label_image_file))\n",
    "        # Set the title of the subplot.\n",
    "        axs[i // 2, i % 2].set_title(\n",
    "            f\"{line_label} ({num_strokes} strokes)\",\n",
    "        )\n",
    "        axs[i // 2, i % 2].title.set_fontsize(15)\n",
    "        # Remove the axis.\n",
    "        axs[i // 2, i % 2].axis(\"off\")\n",
    "\n",
    "    # Hide any unused subplots.\n",
    "    for i in range(num_samples, num_rows * num_cols):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "\n",
    "    # Adjust the spacing between subplots.\n",
    "    plt.subplots_adjust(hspace=0.5, wspace=0.1)\n",
    "\n",
    "    # Display the plot.\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display samples from the train, val_1, val_2, and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 100\n",
    "\n",
    "display_samples_from_files(\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    files=train_cross_val_data_files,\n",
    "    title=\"Train samples\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples_from_files(\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    files=val_1_data_files,\n",
    "    title=\"Val_1 samples\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples_from_files(\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    files=val_2_data_files,\n",
    "    title=\"Val_2 samples\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples_from_files(\n",
    "    num_samples=NUM_SAMPLES,\n",
    "    files=test_data_files,\n",
    "    title=\"Test samples\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
